<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Transformer Theory | Comprehensive Guide to Transformers for Computer Vision Engineers</title>
    <link rel="stylesheet" href="/Comprehensive-Guide-to-Transformers-for-CV/assets/css/style.css?v=ba104ec3b0ab519b06b569f7a0316fb73ba22b23">
    <!-- MathJax support for equations -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- Syntax highlighting -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/github.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>

<!-- Custom favicon -->
<link rel="icon" type="image/png" href="/Comprehensive-Guide-to-Transformers-for-CV/assets/images/favicon.svg">

<!-- Open Graph / Social Media Meta Tags -->
<meta property="og:title" content="Transformer Theory">
<meta property="og:description" content="A comprehensive guide covering transformer theory and applications in computer vision">
<meta property="og:url" content="https://trietvo3105.github.io/Comprehensive-Guide-to-Transformers-for-CV/transformer-theory.html">
<meta property="og:site_name" content="Comprehensive Guide to Transformers for Computer Vision Engineers">
<meta property="og:type" content="website"> 
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Transformer Theory | Comprehensive Guide to Transformers for Computer Vision Engineers</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Transformer Theory" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A comprehensive guide covering transformer theory and applications in computer vision" />
<meta property="og:description" content="A comprehensive guide covering transformer theory and applications in computer vision" />
<link rel="canonical" href="https://trietvo3105.github.io/Comprehensive-Guide-to-Transformers-for-CV/transformer-theory.html" />
<meta property="og:url" content="https://trietvo3105.github.io/Comprehensive-Guide-to-Transformers-for-CV/transformer-theory.html" />
<meta property="og:site_name" content="Comprehensive Guide to Transformers for Computer Vision Engineers" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Transformer Theory" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"A comprehensive guide covering transformer theory and applications in computer vision","headline":"Transformer Theory","url":"https://trietvo3105.github.io/Comprehensive-Guide-to-Transformers-for-CV/transformer-theory.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      <header>
        <h1><a href="https://trietvo3105.github.io/Comprehensive-Guide-to-Transformers-for-CV/">Comprehensive Guide to Transformers for Computer Vision Engineers</a></h1>
        <nav class="main-nav">
          <a href="/Comprehensive-Guide-to-Transformers-for-CV/">Home</a>
          <a href="/Comprehensive-Guide-to-Transformers-for-CV/comprehensive">Overview</a>
          <a href="/Comprehensive-Guide-to-Transformers-for-CV/transformer-theory">Theory</a>
          <a href="/Comprehensive-Guide-to-Transformers-for-CV/transformer-applications">Applications</a>
          <a href="/Comprehensive-Guide-to-Transformers-for-CV/setup-instructions">Setup</a>
          <a href="/Comprehensive-Guide-to-Transformers-for-CV/hands-on-practice">Practice</a>
          <a href="/Comprehensive-Guide-to-Transformers-for-CV/styling-demo">Demo</a>
        </nav>
      </header>
      <main>
        <h1 id="transformer-theory-for-computer-vision">Transformer Theory for Computer Vision</h1>

<h2 id="introduction-to-transformers">Introduction to Transformers</h2>

<p>Transformers have revolutionized the field of deep learning since their introduction in the 2017 paper “Attention is All You Need” by Vaswani et al. Originally designed for natural language processing (NLP) tasks, transformers have since been adapted for computer vision applications with remarkable success. This section covers the foundational theory of transformers, with a focus on their application to computer vision tasks.</p>

<p>The transformer architecture represents a paradigm shift in deep learning, moving away from recurrent and convolutional architectures toward a design that relies entirely on attention mechanisms. This shift has enabled models to capture long-range dependencies more effectively and process data in parallel, leading to significant improvements in performance across various tasks.</p>

<h2 id="the-attention-mechanism-the-core-of-transformers">The Attention Mechanism: The Core of Transformers</h2>

<h3 id="self-attention-explained">Self-Attention Explained</h3>

<p>The key innovation in transformer models is the self-attention mechanism, which allows the model to weigh the importance of different parts of the input data. Unlike recurrent neural networks (RNNs) or convolutional neural networks (CNNs), which process data sequentially or locally, self-attention can directly model relationships between all positions in a sequence.</p>

<p>In the context of transformers, attention mechanisms serve to weigh the influence of different input tokens when producing an output. The self-attention mechanism computes a weighted sum of all tokens in a sequence, where the weights are determined by the compatibility between the query and key representations of the tokens.</p>

<p>To understand self-attention more intuitively, consider how humans read text. When we encounter a pronoun like “it,” we naturally look back at the text to determine what “it” refers to. Similarly, self-attention allows a model to “look” at all other positions in the input to better understand the context of each position. This ability to consider the entire context simultaneously is what gives transformers their power.</p>

<figure>
  <img src="https://private-us-east-1.manuscdn.com/sessionFile/FHzSFfmzQ77Pmg1AmPDaMa/sandbox/BJJmvfzcsiaIHcLFqsolRy-images_1749067736062_na1fn_L2hvbWUvdWJ1bnR1L2ZpbmFsX2ltYWdlcy9zZWxmX2F0dGVudGlvbg.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRkh6U0ZmbXpRNzdQbWcxQW1QRGFNYS9zYW5kYm94L0JKSm12Znpjc2lhSUhjTEZxc29sUnktaW1hZ2VzXzE3NDkwNjc3MzYwNjJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyWnBibUZzWDJsdFlXZGxjeTl6Wld4bVgyRjBkR1Z1ZEdsdmJnLnBuZyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&amp;Key-Pair-Id=K2HSFNDJXOU9YS&amp;Signature=kNP~RlVXJIscjP1IjUbAZeh~K7lbKgo2QXl5rCgvibU03gZ9RZNfrbr9LQ4CWWRUmA48vIULqkLgMVetNMWGu8UjLsm2eWVNCaEZ7WYIu0ENbM3mJeGKzvFloop1LTgglEkgxppsGZjBKXzeOWfuEZMNF1Kt3Y~gvJ6T84G-mV1zbOqHIu1FXNtRD3gE9yaGiJ9MhTgNs20KW6gbkqD3X~lqBHkAhhGkwc0IRuee7CeM7ZbIIsp7XmBnc6CJWKvpLyT6nEYh4GzCCrU-wSTNwqmC3Jmiv08JAwfydNFPsKOUTiWuRK-T76gDePVILYAHeHQli3CZDfpMnRaVIGMG-Q__" alt="Self-Attention Mechanism" width="500" />
  <figcaption>Figure 2: Self-Attention Mechanism showing how Query (Q), Key (K), and Value (V) vectors are derived from input embeddings and interact to produce weighted outputs. The Query represents 'what we're looking for,' the Key represents 'what we have,' and the Value represents 'what we'll return if there's a match.'</figcaption>
</figure>

<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3>

<p>The transformer implements a scaled dot-product attention, which follows these steps:</p>

<ol>
  <li>For each position in the input sequence, create three vectors: Query (Q), Key (K), and Value (V)</li>
  <li>Calculate attention scores by taking the dot product of the query with all keys</li>
  <li>Scale the scores by dividing by the square root of the dimension of the key vectors</li>
  <li>Apply a softmax function to obtain the weights</li>
  <li>Multiply each value vector by its corresponding weight and sum them to produce the output</li>
</ol>

<p>Mathematically, this is represented as:</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \cdot V\]

<p>Where:</p>

<ul>
  <li>\(Q, K, V\) are matrices containing the queries, keys, and values</li>
  <li>\(d_k\) is the dimension of the key vectors</li>
  <li>The scaling factor √d_k prevents the softmax function from having extremely small gradients</li>
</ul>

<p>The scaling factor is crucial because as the dimension of the key vectors increases, the dot products can grow large in magnitude, pushing the softmax function into regions with extremely small gradients. This scaling helps maintain stable gradients during training, allowing the model to learn more effectively.</p>

<h3 id="multi-head-attention">Multi-Head Attention</h3>

<p>To enhance the model’s ability to focus on different positions and representation subspaces, transformers use multi-head attention. This involves:</p>

<ol>
  <li>Linearly projecting the queries, keys, and values multiple times with different learned projections</li>
  <li>Performing the attention function in parallel on each projection</li>
  <li>Concatenating the results and projecting again</li>
</ol>

<p>This allows the model to jointly attend to information from different representation subspaces at different positions, providing a richer understanding of the input data.</p>

<p>Multi-head attention can be thought of as having multiple “representation subspaces” or “attention heads,” each focusing on different aspects of the input. For example, in language processing, one head might focus on syntactic relationships, while another might focus on semantic relationships. By combining these different perspectives, the model gains a more comprehensive understanding of the data.</p>

<figure>
  <img src="https://private-us-east-1.manuscdn.com/sessionFile/FHzSFfmzQ77Pmg1AmPDaMa/sandbox/BJJmvfzcsiaIHcLFqsolRy-images_1749067736062_na1fn_L2hvbWUvdWJ1bnR1L2ZpbmFsX2ltYWdlcy9tdWx0aV9oZWFkX2F0dGVudGlvbg.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRkh6U0ZmbXpRNzdQbWcxQW1QRGFNYS9zYW5kYm94L0JKSm12Znpjc2lhSUhjTEZxc29sUnktaW1hZ2VzXzE3NDkwNjc3MzYwNjJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyWnBibUZzWDJsdFlXZGxjeTl0ZFd4MGFWOW9aV0ZrWDJGMGRHVnVkR2x2YmcucG5nIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzY3MjI1NjAwfX19XX0_&amp;Key-Pair-Id=K2HSFNDJXOU9YS&amp;Signature=DM4qYVzqQQUXebq6lH6KwlOU7qqbvGctM-oetQl8WHEB8G0a41~wVMVitLEgwiRt7Qep~Lu1bu4QeqjE18AFR0~~bIq2-G2c4xv7kbL9R17ciYy-PgesWB2mouFV6-wiwpMYmy9yR48Y~NYJxB4O5~EoO0PtO4tJ1L5njUlXwY-XLoqqFIcn9kZ4tm5~I6eUBs2GGFaeL8LrxuBuRDftNN~51INdG31~LPlAYKOGehv3QgCELnpl7yu-q6e6ikvrVbPC8PRvQvpp9wrYgCpWI6IWSECi6IzsGkSTyr-zhuXEuEr4hw6w87jHq38iQTzbf4xI6eeIBqrgVKJtKXzoHg__" alt="Multi-Head Attention" width="500" />
  <figcaption>Figure 3: Multi-Head Attention architecture showing how multiple attention heads process queries, keys, and values in parallel before merging their outputs. This allows the model to capture different types of relationships simultaneously.</figcaption>
</figure>

<p>The mathematical formulation for multi-head attention is:</p>

\[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, ..., \text{head}_h) \cdot W^O\]

<p>Where:</p>

\[\text{head}_i = \text{Attention}(Q \cdot W_i^Q, K \cdot W_i^K, V \cdot W_i^V)\]

<p>And \(W_i^Q\), \(W_i^K\), \(W_i^V\), and \(W^O\) are learned parameter matrices.</p>

<h2 id="transformer-architecture">Transformer Architecture</h2>

<h3 id="overall-structure">Overall Structure</h3>

<p>A transformer consists of an encoder and a decoder, each composed of multiple identical layers. Each layer has two main components:</p>

<ol>
  <li>A multi-head self-attention mechanism</li>
  <li>A position-wise fully connected feed-forward network</li>
</ol>

<p>Additionally, each sublayer employs residual connections and layer normalization to facilitate training.</p>

<figure>
  <img src="https://private-us-east-1.manuscdn.com/sessionFile/FHzSFfmzQ77Pmg1AmPDaMa/sandbox/BJJmvfzcsiaIHcLFqsolRy-images_1749067736062_na1fn_L2hvbWUvdWJ1bnR1L2ZpbmFsX2ltYWdlcy90cmFuc2Zvcm1lcl9hcmNoaXRlY3R1cmU.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRkh6U0ZmbXpRNzdQbWcxQW1QRGFNYS9zYW5kYm94L0JKSm12Znpjc2lhSUhjTEZxc29sUnktaW1hZ2VzXzE3NDkwNjc3MzYwNjJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyWnBibUZzWDJsdFlXZGxjeTkwY21GdWMyWnZjbTFsY2w5aGNtTm9hWFJsWTNSMWNtVS5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&amp;Key-Pair-Id=K2HSFNDJXOU9YS&amp;Signature=kxNJm4zT8Z1cfJBgb79H7uHx0HqVTbvfJOAsyCr5homKi4fN795TUkAkFJMHg9UPpOQE7PMCMDGCZd4oHXwqKM35YEp~e7YQKqu2q0vvc1menH6rjZYfc6iWx8txO7JP9uuZNoJUW2lmvqe0Ftt~wbKy7fkQafoMMd8QBXA6IYcSE5yV7EIxlNSkfTjyCnyoLaa92E45SNdGF6Gmstni65L7vSl~c3DI81tI4rPDbWNBHp2ygsG7vfElAt1PzlinWxesPWkeiQJBf3gkmlGg7haUtMeDlCPTspNPltq2a2KaC9JnHiO21m6Csv2hD6X8-fe7ViH4rj3M6gAUaxdYDQ__" alt="Transformer Architecture" width="600" />
  <figcaption>Figure 1: Complete Transformer Architecture showing the encoder-decoder structure with multiple stacked layers. The architecture includes token embeddings, positional encodings, and the distinctive multi-head attention mechanisms.</figcaption>
</figure>

<p>The transformer architecture is designed to be highly parallelizable, allowing for efficient training on modern hardware. Unlike RNNs, which process tokens sequentially, transformers can process all tokens in parallel, significantly reducing training time for large datasets.</p>

<h3 id="the-feed-forward-network">The Feed-Forward Network</h3>

<p>The feed-forward network is a critical component in both the encoder and decoder layers of the transformer. It processes each position independently and identically, applying the same transformation to each representation.</p>

<figure>
  <img src="https://private-us-east-1.manuscdn.com/sessionFile/FHzSFfmzQ77Pmg1AmPDaMa/sandbox/BJJmvfzcsiaIHcLFqsolRy-images_1749067736062_na1fn_L2hvbWUvdWJ1bnR1L2ZpbmFsX2ltYWdlcy9mZWVkX2ZvcndhcmRfbmV0d29yaw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRkh6U0ZmbXpRNzdQbWcxQW1QRGFNYS9zYW5kYm94L0JKSm12Znpjc2lhSUhjTEZxc29sUnktaW1hZ2VzXzE3NDkwNjc3MzYwNjJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyWnBibUZzWDJsdFlXZGxjeTltWldWa1gyWnZjbmRoY21SZmJtVjBkMjl5YXcucG5nIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzY3MjI1NjAwfX19XX0_&amp;Key-Pair-Id=K2HSFNDJXOU9YS&amp;Signature=iVMH2etXatHaCQMcFGjSpJbVJth4PMti7qL7mj4IioiTDEhVKtHDw~cjkgD092zmXi8eV6wCZVyZFJe8q5vNgGkC642rAKSbNIu2qZIHWyK4FQXq64dzREbF67CLP6ZKwMfvnX2roXwYslDFOD9TPb1SMY7JqdyvGRfaUHgOb0ovc~IaxFfgmfehDdvnVFMQRbO17zF2kzkj8iE51Gc9OWOZ76WKYI-Niu5AUzmDNUH5lrSDGdsJpy-bTa8~7EbsCBQgrvZf2WNNNuEBXpeuV80n6P0cbc~QTH8T5gW44IMfkcOFPtlo2YRBr5JmuG3PkcKa-6Al2PR-c6pFi~KNKQ__" alt="Feed-Forward Network" width="450" />
  <figcaption>Figure 2: Position-wise Feed-Forward Network in the context of the transformer architecture. This component applies identical transformations to each position independently after the attention mechanism has captured contextual relationships.</figcaption>
</figure>

<p>This component serves several important purposes:</p>

<ol>
  <li>It adds non-linearity to the model, allowing it to learn more complex patterns</li>
  <li>It transforms the attention-weighted representations into a form suitable for the next layer</li>
  <li>It increases the model’s capacity by introducing additional parameters</li>
</ol>

<p>The dimensionality of the inner layer (between \(W_1\) and \(W_2\)) is typically larger than the model dimension, often by a factor of 4. This expansion and subsequent projection allow the network to capture more complex relationships before compressing the information back to the model dimension.</p>

<h3 id="the-encoder">The Encoder</h3>

<p>The encoder processes the input sequence and generates representations that capture the contextual relationships within the data. It transforms the input tokens into a continuous representation that encodes the full context of the sequence.</p>

<figure>
  <img src="https://private-us-east-1.manuscdn.com/sessionFile/FHzSFfmzQ77Pmg1AmPDaMa/sandbox/BJJmvfzcsiaIHcLFqsolRy-images_1749067736062_na1fn_L2hvbWUvdWJ1bnR1L2ZpbmFsX2ltYWdlcy9lbmNvZGVyX2FyY2hpdGVjdHVyZQ.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRkh6U0ZmbXpRNzdQbWcxQW1QRGFNYS9zYW5kYm94L0JKSm12Znpjc2lhSUhjTEZxc29sUnktaW1hZ2VzXzE3NDkwNjc3MzYwNjJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyWnBibUZzWDJsdFlXZGxjeTlsYm1OdlpHVnlYMkZ5WTJocGRHVmpkSFZ5WlEucG5nIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzY3MjI1NjAwfX19XX0_&amp;Key-Pair-Id=K2HSFNDJXOU9YS&amp;Signature=VRUU67bvyHWvkle5JN5z8d37TnX9ahguE7qP2BVuDuiE7i8WIbXJk9csGFtX7fSRwGL9A5N9fH0qTdJsgctiYxSnu~CscY-DOhEPyYJtk9c86DUJx~uTy0O4Vn84kZ8MWGhNm9Z0g89jL0rrRgZpSFt1LAioc-WkfE~oZorQEkhmnyQ0CRWaa3Dte9MyV6KKfiN~9qIy3MGpjQmHf~a7pTOqBX-H1u~8P5gvT7PJ0vkRxuxxNKGb~X2Ramc0XIkpfy3424XjZbaWvkUHW66xVP6RwJZZpwh9WlObiB0vRfv6c0APAP9CIAGCf~b-9Wn7AXQjkTy3A~S-EqvtwiZoyw__" alt="Encoder Architecture" width="400" />
  <figcaption>Figure 3: Transformer Encoder Block showing the multi-head attention mechanism, position-wise feed-forward network, and the residual connections with layer normalization.</figcaption>
</figure>

<p>Each encoder layer consists of two main sub-layers:</p>

<ol>
  <li>
    <p><strong>Multi-Head Self-Attention Layer</strong>: This mechanism allows each position to attend to all positions in the previous layer. The self-attention operation enables the encoder to weigh the importance of different tokens when creating the representation for each position. For example, when encoding the word “bank” in the sentence “I went to the bank to deposit money,” the self-attention mechanism would give higher weight to words like “deposit” and “money” to understand that “bank” refers to a financial institution rather than a riverbank.</p>
  </li>
  <li>
    <p><strong>Position-wise Feed-Forward Network</strong>: After the attention mechanism, each position passes independently through an identical feed-forward network. This network consists of two linear transformations with a ReLU activation in between:</p>

\[\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2\]

    <p>This component adds non-linearity to the model and allows it to transform the attention-weighted representations further.</p>
  </li>
  <li>
    <p><strong>Layer Normalization and Residual Connections</strong>: Each sub-layer is wrapped with a residual connection followed by layer normalization. The residual connections (shown in red in the diagram) help with gradient flow during training, while layer normalization stabilizes the learning process by normalizing the inputs across features.</p>
  </li>
</ol>

<p>The encoder’s self-attention mechanism allows it to create contextual representations of each input token, taking into account the entire input sequence. This is particularly valuable for tasks where understanding the relationships between different parts of the input is crucial. Multiple encoder layers are typically stacked on top of each other, with each layer refining the representations from the previous layer.</p>

<h3 id="the-decoder">The Decoder</h3>

<p>The decoder generates the output sequence one element at a time in an autoregressive manner. This component is responsible for transforming the encoder’s representations into the final output sequence, such as translated text in a machine translation task.</p>

<figure>
  <img src="https://private-us-east-1.manuscdn.com/sessionFile/FHzSFfmzQ77Pmg1AmPDaMa/sandbox/BJJmvfzcsiaIHcLFqsolRy-images_1749067736062_na1fn_L2hvbWUvdWJ1bnR1L2ZpbmFsX2ltYWdlcy9uZXdfZGVjb2Rlcl9hcmNoaXRlY3R1cmU.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRkh6U0ZmbXpRNzdQbWcxQW1QRGFNYS9zYW5kYm94L0JKSm12Znpjc2lhSUhjTEZxc29sUnktaW1hZ2VzXzE3NDkwNjc3MzYwNjJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyWnBibUZzWDJsdFlXZGxjeTl1WlhkZlpHVmpiMlJsY2w5aGNtTm9hWFJsWTNSMWNtVS5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NjcyMjU2MDB9fX1dfQ__&amp;Key-Pair-Id=K2HSFNDJXOU9YS&amp;Signature=vfCYupPay0NTjry9G0P2lWBDIyD2V-0PUW4kVccrf656zFGu0T1BmqYWGiwn3Zo0fs42F8R8MROwKAaFsrlBe890S4utwjRa-XaE75aIhN1rQBECs0QeQi78s5QgO2Z78inCvpSrHnNJeFM8CaV-actJJ0c3fGtNb6VXcN40tvseY4Gp0io-FJGL1XmyrUqdWhW8Lpcr19daLvc2dan56aACG3X5NgU0E~tjZ4MNWe9XXRmXv5abwB-MqOdHYwUnDK4bCoZ2BzT2RTpVCQ2RtjBhtspH9msyNSXLlFVVMaeZwRg8fNPI91I9x9~~tmmE7mZ45KVos~syJcC2sYv8KQ__" alt="Decoder Architecture" width="400" />
  <figcaption>Figure 4: Transformer Decoder Block showing the masked self-attention, encoder-decoder attention, and feed-forward layers with their respective normalization. Note the visual consistency with the encoder block architecture.</figcaption>
</figure>

<p>Each decoder layer includes three main sub-layers:</p>

<ol>
  <li>
    <p><strong>Masked Multi-Head Self-Attention</strong>: This first attention mechanism operates on the decoder’s own outputs from the previous layer. The “masked” aspect is crucial - it prevents positions from attending to subsequent positions by applying a mask to the attention scores before the softmax operation. This masking ensures that predictions for position i can only depend on known outputs at positions less than i, preserving the autoregressive property necessary for generation tasks.</p>
  </li>
  <li>
    <p><strong>Multi-Head Encoder-Decoder Attention</strong>: This second attention layer is what connects the decoder to the encoder. Here, queries come from the decoder’s previous layer, while keys and values come from the encoder’s output. This allows each position in the decoder to attend to all positions in the input sequence, creating a direct information pathway from input to output. This cross-attention mechanism is what allows the model to focus on relevant parts of the source sequence when generating each target token.</p>
  </li>
  <li>
    <p><strong>Position-wise Feed-Forward Network</strong>: Identical to the one in the encoder, this fully connected feed-forward network applies the same transformation to each position independently.</p>
  </li>
</ol>

<p>Each of these sub-layers is followed by a residual connection and layer normalization, similar to the encoder. This combination helps maintain stable gradients during training and allows for deeper networks.</p>

<h4 id="decoder-operation-in-detail">Decoder Operation in Detail</h4>

<p>The decoder operates sequentially during inference, generating one token at a time:</p>

<ol>
  <li>The decoder starts with a special start-of-sequence token and positional encoding.</li>
  <li>For each new token position:</li>
</ol>

<ul>
  <li>The masked self-attention ensures the decoder only considers previously generated tokens.</li>
  <li>The encoder-decoder attention allows the decoder to focus on relevant parts of the input sequence.</li>
  <li>The feed-forward network and final linear layer transform the representations into output probabilities.</li>
  <li>The token with the highest probability is selected (or sampling is used) and added to the sequence.</li>
  <li>This process repeats until an end-of-sequence token is generated or a maximum length is reached.</li>
</ul>

<p>The decoder’s masked self-attention is what enables autoregressive generation. By masking future positions during training, the model learns to predict the next token based only on previously generated tokens, which is essential for tasks like machine translation or text generation.</p>

<p>The encoder-decoder attention mechanism serves as a dynamic, content-based alignment between input and output sequences. Unlike traditional sequence-to-sequence models with fixed attention mechanisms, this dynamic attention allows the model to adaptively focus on different parts of the input depending on what it’s currently generating, leading to more accurate and contextually appropriate outputs.</p>

<h3 id="positional-encoding">Positional Encoding</h3>

<p>Since transformers do not inherently process sequential information, positional encodings are added to the input embeddings to provide information about the relative or absolute position of tokens in the sequence. These encodings have the same dimension as the embeddings, allowing them to be summed.</p>

<figure>
  <img src="https://private-us-east-1.manuscdn.com/sessionFile/FHzSFfmzQ77Pmg1AmPDaMa/sandbox/BJJmvfzcsiaIHcLFqsolRy-images_1749067736062_na1fn_L2hvbWUvdWJ1bnR1L2ZpbmFsX2ltYWdlcy9wb3NpdGlvbmFsX2VuY29kaW5n.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRkh6U0ZmbXpRNzdQbWcxQW1QRGFNYS9zYW5kYm94L0JKSm12Znpjc2lhSUhjTEZxc29sUnktaW1hZ2VzXzE3NDkwNjc3MzYwNjJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyWnBibUZzWDJsdFlXZGxjeTl3YjNOcGRHbHZibUZzWDJWdVkyOWthVzVuLnBuZyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&amp;Key-Pair-Id=K2HSFNDJXOU9YS&amp;Signature=riN3qcgkRyFFSW8nfLppv8sniPVG-s7D9MN34inH35LEEynPa4Nqz5QxzmR5Au8OYZvWKsdEqNnbhq9Fd85Ptv-GDJOP7Td5Fww7kt3-nJYdQ-52v6rgk3x4G1xgScPT4pavuNSzmIqmNw3Pdxs1X5yUJo-9y7XwcfnKEQUqxtihyEyLO9nJf~UjGbYvkSU~BAbnWiCaAa2IJABCLBgUzP1BKxH2ObBUOBU0yI~n0xCMkGSVAC8ucw2pJs01684Rt9Arb7J22NG0aRbSeFyZvqfaQznhKzjO-OTggOq27tKrkgPIe2l2mpqJ9FI9GhTQmDk~xARmgJCWsioAUTcbxQ__" alt="Positional Encoding" width="600" />
  <figcaption>Figure 5: Visualization of sinusoidal positional encoding showing how different dimensions use sine and cosine functions of different frequencies. This creates a unique pattern for each position that allows the model to understand sequence order.</figcaption>
</figure>

<p>The original transformer paper used sine and cosine functions of different frequencies to create these positional encodings:</p>

\[PE(pos, 2i) = \sin(pos / 10000^{(2i/d_{\text{model}})}) \\
PE(pos, 2i+1) = \cos(pos / 10000^{(2i/d_{\text{model}})})\]

<p>Where:</p>

<ul>
  <li>\(pos\) is the position</li>
  <li>\(i\) is the dimension</li>
  <li>\(d_{\text{model}}\) is the embedding dimension</li>
</ul>

<p>These encodings have the useful property that for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos), which helps the model learn relative positions more easily.</p>

<p>Positional encodings are crucial for transformers because, unlike RNNs or CNNs, the self-attention operation is permutation-invariant—it doesn’t inherently know the order of the input sequence. By adding positional information to the token embeddings, the model can distinguish between tokens at different positions and understand the sequential nature of the data.</p>

<h2 id="from-nlp-to-computer-vision-vision-transformers-vit">From NLP to Computer Vision: Vision Transformers (ViT)</h2>

<h3 id="adapting-transformers-for-images">Adapting Transformers for Images</h3>

<p>Vision Transformers (ViT) adapt the transformer architecture for image processing tasks. The key insight is to treat an image as a sequence of patches, similar to how words are treated in NLP applications.</p>

<p>The process involves:</p>

<ol>
  <li>Splitting the image into fixed-size patches (typically 16×16 pixels)</li>
  <li>Flattening each patch into a vector</li>
  <li>Linearly projecting these vectors to obtain patch embeddings</li>
  <li>Adding position embeddings to retain spatial information</li>
  <li>Processing the resulting sequence through a standard transformer encoder</li>
</ol>

<figure>
  <img src="https://private-us-east-1.manuscdn.com/sessionFile/FHzSFfmzQ77Pmg1AmPDaMa/sandbox/BJJmvfzcsiaIHcLFqsolRy-images_1749067736062_na1fn_L2hvbWUvdWJ1bnR1L2ZpbmFsX2ltYWdlcy92aXNpb25fdHJhbnNmb3JtZXI.webp?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRkh6U0ZmbXpRNzdQbWcxQW1QRGFNYS9zYW5kYm94L0JKSm12Znpjc2lhSUhjTEZxc29sUnktaW1hZ2VzXzE3NDkwNjc3MzYwNjJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyWnBibUZzWDJsdFlXZGxjeTkyYVhOcGIyNWZkSEpoYm5ObWIzSnRaWEkud2VicCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzIyNTYwMH19fV19&amp;Key-Pair-Id=K2HSFNDJXOU9YS&amp;Signature=IJvLI932hA9Ky~5ARAoBXCutFbZ10YWl1IZJLhHmu2ynpThChowYqcl7YW-jZuBR2ca5TMWs1ZHQ9UWjhlmh7geNpzfkrhHVU0KS6gqVYvSf5DSL-sLFwOyD-NCF~GkXxL6W6QmBX8Jy5sskx-N2~tIznmQBUx~bUYASXUiMie376z7Ns2gVpc7CuGSWF2Lyu-KsEncAyShYmm-cHaRpgqEFRlNgQQjZjEg1r-Y5LFfqVxCF3LrS3UjSa6uq29RXmXLVVa7Lx5mfXyODFDUGOGJ88LXBIXvF~pT4i2BBVmU1LqnFhEYw-O0O-ibvWv6~CeH90WKZHCiAGANUnLV02A__" alt="Vision Transformer Architecture" width="550" />
  <figcaption>Figure 6: Vision Transformer (ViT) architecture showing how an image is divided into patches, which are then linearly embedded, combined with position embeddings, and processed through a transformer encoder.</figcaption>
</figure>

<p>This approach differs significantly from traditional CNN architectures, which use convolutional layers to process images hierarchically. The self-attention mechanism in transformers allows ViT to capture global dependencies directly, without the need for multiple layers of convolution and pooling to increase the receptive field.</p>

<h3 id="the-vit-architecture-in-detail">The ViT Architecture in Detail</h3>

<p>The Vision Transformer architecture consists of the following components:</p>

<ol>
  <li>
    <p><strong>Patch Embedding</strong>: The input image is divided into non-overlapping patches, which are then flattened and linearly projected to create patch embeddings. For example, a 224×224 pixel image divided into 16×16 patches would result in 196 patches (14×14), each represented as a vector.</p>
  </li>
  <li>
    <p><strong>Class Token</strong>: A learnable embedding (CLS token) is prepended to the sequence of patch embeddings. The final representation of this token is used for image classification tasks.</p>
  </li>
  <li>
    <p><strong>Position Embedding</strong>: Since transformers have no inherent understanding of spatial relationships, position embeddings are added to the patch embeddings to provide information about the spatial arrangement of patches.</p>
  </li>
  <li>
    <p><strong>Transformer Encoder</strong>: The resulting sequence of embeddings is processed through multiple layers of a standard transformer encoder, which consists of multi-head self-attention and MLP blocks, with layer normalization and residual connections.</p>
  </li>
  <li>
    <p><strong>MLP Head</strong>: For classification tasks, final representation of the CLS token is passed through an MLP head to produce class predictions.</p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                 <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">patch_embed</span> <span class="o">=</span> <span class="n">PatchEmbed</span><span class="p">(</span><span class="n">img_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">in_chans</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="n">num_patches</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">patch_embed</span><span class="p">.</span><span class="n">num_patches</span>

        <span class="c1"># Create class token and position embeddings
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>

        <span class="c1"># Transformer encoder
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">Block</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="c1"># MLP head for classification
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="key-innovations-in-vision-transformers">Key Innovations in Vision Transformers</h3>

<p>Vision Transformers introduced several key innovations that enabled transformers to be effectively applied to computer vision tasks:</p>

<ol>
  <li>
    <p><strong>Patch-based Image Representation</strong>: By treating image patches as tokens, ViT adapts the transformer architecture to process visual data without significantly modifying the original transformer design.</p>
  </li>
  <li>
    <p><strong>Pre-training on Large Datasets</strong>: ViT demonstrated that with sufficient pre-training data (e.g., JFT-300M), transformers can outperform CNNs on image classification tasks without incorporating image-specific inductive biases.</p>
  </li>
  <li>
    <p><strong>Transfer Learning</strong>: Pre-trained ViT models can be effectively fine-tuned on smaller datasets, making them practical for a wide range of computer vision applications.</p>
  </li>
</ol>

<p>Despite their success, early Vision Transformers had some limitations:</p>

<ol>
  <li>They required large amounts of training data to achieve competitive performance compared to CNNs.</li>
  <li>The computational complexity of self-attention (quadratic in the number of patches) limited their application to high-resolution images.</li>
  <li>They lacked some of the inductive biases that make CNNs effective for vision tasks, such as translation equivariance and locality.</li>
</ol>

<p>These limitations have been addressed in subsequent transformer-based vision models.</p>

<h3 id="advancements-beyond-basic-vit">Advancements Beyond Basic ViT</h3>

<p>Since the introduction of the original Vision Transformer, numerous advancements have been made to address its limitations and improve its performance:</p>

<ol>
  <li>
    <p><strong>Hierarchical Transformers</strong>: Models like Swin Transformer introduce a hierarchical structure with local attention windows, making them more efficient and effective for high-resolution images and dense prediction tasks.</p>
  </li>
  <li>
    <p><strong>Hybrid Architectures</strong>: Combining convolutional layers with transformer blocks to leverage the strengths of both approaches, as seen in models like ConViT and CvT.</p>
  </li>
  <li>
    <p><strong>Efficient Attention Mechanisms</strong>: Developing more efficient attention variants to reduce the computational complexity of self-attention, such as linear attention and sparse attention.</p>
  </li>
  <li>
    <p><strong>Data-efficient Training</strong>: Techniques like DeiT (Data-efficient image Transformers) show that with appropriate training strategies, ViTs can be trained effectively on smaller datasets without extensive pre-training.</p>
  </li>
</ol>

<p>Vision Transformers have revolutionized computer vision by demonstrating that the transformer architecture, originally designed for NLP tasks, can be highly effective for visual tasks as well. This cross-domain success has led to a convergence of techniques across modalities and inspired a new generation of models that leverage the strengths of both transformers and CNNs.</p>

<p>The rapid advancement of transformer-based vision models continues to push the state-of-the-art in computer vision, with applications ranging from image classification and object detection to segmentation, video understanding, and multimodal learning.</p>

<h2 id="advanced-transformer-variants-for-computer-vision">Advanced Transformer Variants for Computer Vision</h2>

<h3 id="swin-transformer">Swin Transformer</h3>

<p>The Swin (Shifted Window) Transformer introduces hierarchical feature maps and shifted windowing to improve efficiency and performance. Key innovations include:</p>

<ol>
  <li>
    <p><strong>Hierarchical Feature Maps</strong>: Merging patches progressively to create a hierarchical representation. This is similar to the feature pyramid in CNNs and allows the model to capture multi-scale features efficiently. At each stage, neighboring patches are merged, reducing the sequence length while increasing the feature dimension.</p>
  </li>
  <li>
    <p><strong>Window-based Self-Attention</strong>: Computing self-attention within local windows to reduce computational complexity. Instead of performing self-attention across the entire image, which scales quadratically with image size, Swin Transformer restricts attention to local windows. This reduces the computational complexity to linear with respect to image size.</p>
  </li>
  <li>
    <p><strong>Shifted Window Partitioning</strong>: Alternating between regular and shifted window partitioning to enable cross-window connections. In one layer, the image is divided into non-overlapping windows; in the next layer, the window partitioning is shifted, creating connections between previously separate windows. This allows information to flow across the entire image while maintaining computational efficiency.</p>
  </li>
</ol>

<p>The Swin Transformer’s hierarchical design and efficient attention mechanism make it particularly well-suited for dense prediction tasks like object detection and semantic segmentation, where multi-scale feature representations are important.</p>

<h3 id="deit-data-efficient-image-transformers">DeiT (Data-efficient image Transformers)</h3>

<p>DeiT demonstrates that Vision Transformers can be trained effectively on smaller datasets through:</p>

<ol>
  <li>
    <p><strong>Distillation Token</strong>: A specific token that learns from a teacher model (typically a CNN). In addition to the class token, DeiT introduces a distillation token that is supervised by the output of a pre-trained CNN. This allows the model to leverage the inductive biases of CNNs without explicitly incorporating them into the architecture.</p>
  </li>
  <li>
    <p><strong>Strong Data Augmentation</strong>: Techniques like RandAugment and CutMix to increase data diversity. These augmentations create a more varied training set, helping the model generalize better from limited data. RandAugment applies a series of random transformations to each image, while CutMix combines patches from different images along with their labels.</p>
  </li>
  <li>
    <p><strong>Regularization</strong>: Methods to prevent overfitting on smaller datasets. DeiT employs techniques like stochastic depth (randomly dropping layers during training) and weight decay to improve generalization.</p>
  </li>
</ol>

<p>DeiT’s approach shows that with appropriate training techniques, transformers can achieve competitive performance even without the massive pre-training datasets used in the original ViT paper.</p>

<h3 id="mobilevit">MobileViT</h3>

<p>MobileViT combines the strengths of CNNs and transformers for mobile applications:</p>

<ol>
  <li>
    <p><strong>Local Processing</strong>: Using convolutions for local feature extraction. MobileViT starts with convolutional layers to efficiently capture local patterns and reduce the spatial dimensions of the input.</p>
  </li>
  <li>
    <p><strong>Global Understanding</strong>: Applying transformers for global context modeling. After the initial convolutional processing, transformer blocks are used to capture long-range dependencies and global context.</p>
  </li>
  <li>
    <p><strong>Lightweight Design</strong>: Optimized architecture for mobile devices with limited computational resources. MobileViT uses depth-wise separable convolutions and efficient transformer implementations to reduce the number of parameters and computational cost.</p>
  </li>
</ol>

<p>This hybrid approach leverages the strengths of both architectures—the efficiency and local processing capabilities of CNNs, and the global modeling capabilities of transformers—making it well-suited for deployment on resource-constrained devices.</p>

<h2 id="attention-mechanisms-beyond-transformers">Attention Mechanisms Beyond Transformers</h2>

<h3 id="non-local-neural-networks">Non-local Neural Networks</h3>

<p>Non-local neural networks capture long-range dependencies by computing the response at a position as a weighted sum of features at all positions. This is conceptually similar to self-attention but was developed independently.</p>

<p>The non-local operation can be expressed as:</p>

\[y_i = \frac{1}{C(x)} \sum_j f(x_i, x_j) g(x_j)\]

<p>Where:</p>

<ul>
  <li>\(x_i\) is the input feature at position i</li>
  <li>\(y_i\) is the output feature at position i</li>
  <li>\(f(x_i, x_j)\) computes a scalar representing the relationship between positions i and j</li>
  <li>\(g(x_j)\) computes a representation of the input at position j</li>
  <li>\(C(x)\) is a normalization factor</li>
</ul>

<p>This operation allows the model to consider all positions when computing the output for each position, capturing global context. Non-local neural networks have been successfully applied to video understanding, where capturing temporal dependencies is crucial.</p>

<h3 id="squeeze-and-excitation-networks">Squeeze-and-Excitation Networks</h3>

<p>Squeeze-and-Excitation networks adaptively recalibrate channel-wise feature responses by explicitly modeling interdependencies between channels, providing a form of attention over feature channels.</p>

<p>The SE block consists of two operations:</p>

<ol>
  <li><strong>Squeeze</strong>: Global average pooling to extract channel-wise statistics</li>
  <li><strong>Excitation</strong>: A gating mechanism with two fully connected layers and a sigmoid activation to generate channel-wise attention weights</li>
</ol>

<p>Mathematically, for input feature map U:</p>

\[z = F_{\text{sq}}(U) = \text{GlobalAveragePooling}(U) \\
s = F_{\text{ex}}(z) = \sigma(W_2 \cdot \text{ReLU}(W_1 \cdot z)) \\
\hat{U} = s \cdot U\]

<p>Where:</p>

<ul>
  <li>\(z\) is the squeezed representation</li>
  <li>\(s\) is the channel-wise attention weights</li>
  <li>\(\hat{U}\) is the recalibrated feature map</li>
  <li>\(W_1\) and \(W_2\) are learnable parameters</li>
</ul>

<p>SE networks improve performance by allowing the model to emphasize informative features and suppress less useful ones, with minimal additional computational cost.</p>

<h3 id="cbam-convolutional-block-attention-module">CBAM (Convolutional Block Attention Module)</h3>

<p>CBAM combines channel and spatial attention to enhance the representational power of CNNs, focusing on “what” and “where” to attend in the feature maps.</p>

<p>CBAM consists of two sequential sub-modules:</p>

<ol>
  <li><strong>Channel Attention Module</strong>: Similar to SE networks, it focuses on “what” is meaningful in the input features</li>
  <li><strong>Spatial Attention Module</strong>: Focuses on “where” to attend by generating a spatial attention map</li>
</ol>

<p>The process can be summarized as:</p>

\[F' = M_c(F) \otimes F \\
F'' = M_s(F') \otimes F'\]

<p>Where:</p>

<ul>
  <li>\(F\) is the input feature map</li>
  <li>\(M_c\) is the channel attention map</li>
  <li>\(M_s\) is the spatial attention map</li>
  <li>⊗ represents element-wise multiplication</li>
</ul>

<p>CBAM provides a complementary attention mechanism to the self-attention used in transformers, focusing on different aspects of the input data.</p>

<h2 id="practical-applications-of-vision-transformers">Practical Applications of Vision Transformers</h2>

<p>Vision Transformers have demonstrated impressive performance across a wide range of computer vision tasks:</p>

<ol>
  <li>
    <p><strong>Image Classification</strong>: ViTs have achieved state-of-the-art results on benchmark datasets like ImageNet, particularly when pre-trained on large datasets.</p>
  </li>
  <li>
    <p><strong>Object Detection</strong>: Models like DETR (DEtection TRansformer) use transformers to directly predict object bounding boxes and classes, eliminating the need for hand-designed components like anchor boxes and non-maximum suppression.</p>
  </li>
  <li>
    <p><strong>Semantic Segmentation</strong>: Transformer-based models can generate pixel-level predictions for scene understanding, benefiting from their ability to capture long-range dependencies.</p>
  </li>
  <li>
    <p><strong>Image Generation</strong>: Models like DALL-E and Stable Diffusion use transformer architectures to generate high-quality images from text descriptions, showcasing the versatility of transformers beyond discriminative tasks.</p>
  </li>
  <li>
    <p><strong>Video Understanding</strong>: Transformers can process spatio-temporal data for tasks like action recognition and video captioning, leveraging their ability to model relationships across both space and time.</p>
  </li>
  <li>
    <p><strong>Multi-modal Learning</strong>: Transformers excel at integrating information from different modalities, such as text and images, enabling applications like visual question answering and image captioning.</p>
  </li>
</ol>

<h2 id="glossary-of-key-terms">Glossary of Key Terms</h2>

<ul>
  <li><strong>Attention</strong>: A mechanism that allows models to focus on relevant parts of the input when producing an output.</li>
  <li><strong>Self-Attention</strong>: A specific form of attention where the query, key, and value all come from the same source.</li>
  <li><strong>Query, Key, Value (Q, K, V)</strong>: The three components used in attention mechanisms. The query represents what we’re looking for, the key represents what we have, and the value represents what we’ll return if there’s a match.</li>
  <li><strong>Multi-Head Attention</strong>: Running multiple attention operations in parallel and concatenating the results.</li>
  <li><strong>Encoder</strong>: The part of the transformer that processes the input sequence and generates representations.</li>
  <li><strong>Decoder</strong>: The part of the transformer that generates the output sequence based on the encoder’s representations.</li>
  <li><strong>Positional Encoding</strong>: Information added to token embeddings to provide the model with information about the position of each token.</li>
  <li><strong>Vision Transformer (ViT)</strong>: An adaptation of the transformer architecture for image processing tasks.</li>
  <li><strong>Patch Embedding</strong>: The process of dividing an image into patches and projecting them into a high-dimensional space.</li>
  <li><strong>Class Token</strong>: A special token added to the sequence of patch embeddings in ViT, used for classification tasks.</li>
  <li><strong>Inductive Bias</strong>: The set of assumptions that a model makes to generalize from training data to unseen data.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Transformers have fundamentally changed how we approach computer vision tasks. By leveraging self-attention mechanisms, they can capture global dependencies and relationships within images more effectively than traditional architectures. Vision Transformers and their variants have demonstrated state-of-the-art performance across various computer vision tasks, from image classification to object detection and segmentation.</p>

<p>The success of transformers in computer vision highlights the value of cross-pollination between different domains of deep learning. As research continues, we can expect further innovations that combine the strengths of transformers with other architectural paradigms, leading to more efficient and effective models for computer vision applications.</p>

<h2 id="references">References</h2>

<ol>
  <li>
    <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems.</p>
  </li>
  <li>
    <p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … &amp; Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations.</p>
  </li>
  <li>
    <p>Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., … &amp; Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision.</p>
  </li>
  <li>
    <p>Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., &amp; Jégou, H. (2021). Training data-efficient image transformers &amp; distillation through attention. In International Conference on Machine Learning.</p>
  </li>
  <li>
    <p>Mehta, S., &amp; Rastegari, M. (2021). MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer. arXiv preprint arXiv:2110.02178.</p>
  </li>
  <li>
    <p>Wang, X., Girshick, R., Gupta, A., &amp; He, K. (2018). Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition.</p>
  </li>
  <li>
    <p>Hu, J., Shen, L., &amp; Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition.</p>
  </li>
  <li>
    <p>Woo, S., Park, J., Lee, J. Y., &amp; Kweon, I. S. (2018). Cbam: Convolutional block attention module. In Proceedings of the European conference on computer vision.</p>
  </li>
  <li>
    <p>Alammar, J. (2018). The Illustrated Transformer. https://jalammar.github.io/illustrated-transformer/</p>
  </li>
  <li>
    <p>Machine Learning Mastery. (2023). The Transformer Attention Mechanism. https://www.machinelearningmastery.com/the-transformer-attention-mechanism/</p>
  </li>
</ol>


      </main>
      <footer>
        <p>&copy; 2025. All rights reserved.</p>
      </footer>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
  </body>
</html> 